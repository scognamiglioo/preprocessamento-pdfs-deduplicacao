{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "nBzRmiVFRvG6"
      },
      "outputs": [],
      "source": [
        "!pip install pdfplumber -q\n",
        "!pip install PyPDF2 -q\n",
        "!pip install python-Levenshtein -q\n",
        "!pip install scikit-learn -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "HUgv_6NyOKk5"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import json\n",
        "import hashlib\n",
        "from typing import List, Dict, Any, Optional\n",
        "from dataclasses import dataclass\n",
        "from datetime import datetime\n",
        "import unicodedata\n",
        "from difflib import SequenceMatcher\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "from PyPDF2 import PdfReader\n",
        "import pdfplumber"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FjAZRlWTIUfh",
        "outputId": "96e0378c-0b72-4c1a-964b-a954ca045928"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "\n",
            " Processando: REGULAMENTO TCC BSI\n",
            "PROCESSANDO DOCUMENTO COMPLETO: REGULAMENTO TCC BSI\n",
            "Extraindo todas as p√°ginas com texto e tabelas\n",
            "Processando 13 p√°ginas...\n",
            "     P√°ginas processadas: 10/13\n",
            "     Extra√ß√£o conclu√≠da: 13 p√°ginas\n",
            "  Fase 1 - Duplicatas exatas: 0 removidas\n",
            "  Removendo p√°gina 12 (similar √† p√°gina 11)\n",
            "  Fase 2 - P√°ginas similares: 1 removidas\n",
            "Processamento conclu√≠do: 12 p√°ginas √∫nicas\n",
            "Conclu√≠do: REGULAMENTO TCC BSI\n",
            "\n",
            " Processando: Estatuto - Setembro de 2025\n",
            "PROCESSANDO DOCUMENTO COMPLETO: Estatuto - Setembro de 2025\n",
            "Extraindo todas as p√°ginas com texto e tabelas\n",
            "Processando 54 p√°ginas...\n",
            "     P√°ginas processadas: 10/54\n",
            "     P√°ginas processadas: 20/54\n",
            "     P√°ginas processadas: 30/54\n",
            "     P√°ginas processadas: 40/54\n",
            "     P√°ginas processadas: 50/54\n",
            "     Extra√ß√£o conclu√≠da: 54 p√°ginas\n",
            "  Fase 1 - Duplicatas exatas: 0 removidas\n",
            "  Fase 2 - P√°ginas similares: 0 removidas\n",
            "Processamento conclu√≠do: 54 p√°ginas √∫nicas\n",
            "Conclu√≠do: Estatuto - Setembro de 2025\n",
            "\n",
            " Processando: REGULAMENTO DAS ATIVIDADES COMPLEMENTARES\n",
            "PROCESSANDO DOCUMENTO COMPLETO: REGULAMENTO DAS ATIVIDADES COMPLEMENTARES\n",
            "Extraindo todas as p√°ginas com texto e tabelas\n",
            "Processando 8 p√°ginas...\n",
            "     Extra√ß√£o conclu√≠da: 8 p√°ginas\n",
            "  Fase 1 - Duplicatas exatas: 0 removidas\n",
            "  Fase 2 - P√°ginas similares: 0 removidas\n",
            "Processamento conclu√≠do: 8 p√°ginas √∫nicas\n",
            "Conclu√≠do: REGULAMENTO DAS ATIVIDADES COMPLEMENTARES\n",
            "\n",
            "======================================================================\n",
            "======================================================================\n",
            "\n",
            " REGULAMENTO TCC BSI\n",
            "    P√°ginas processadas: 12\n",
            "    Tabelas extra√≠das: 3\n",
            "    ID: TCC_BSI_2024_001\n",
            "    Vers√£o: 1.0 - 2024-01-15\n",
            "\n",
            " Estatuto - Setembro de 2025\n",
            "    P√°ginas processadas: 54\n",
            "    Tabelas extra√≠das: 1\n",
            "    ID: ESTATUTO_2025_001\n",
            "    Vers√£o: 2.0 - 2025-09-01\n",
            "\n",
            " REGULAMENTO DAS ATIVIDADES COMPLEMENTARES\n",
            "    P√°ginas processadas: 8\n",
            "    Tabelas extra√≠das: 6\n",
            "    ID: ACC_2024_001\n",
            "    Vers√£o: 1.1 - 2024-03-20\n",
            "\n",
            " TOTAL GERAL:\n",
            "    Documentos: 3\n",
            "    P√°ginas: 74\n",
            "    Tabelas: 10\n",
            "\n",
            "Arquivo salvo: output.jsonl\n"
          ]
        }
      ],
      "source": [
        "@dataclass\n",
        "class PageData:\n",
        "    page: int\n",
        "    raw_text: str\n",
        "    normalized_text: str\n",
        "    chapter: Optional[str]\n",
        "    article: Optional[str]\n",
        "    tables: List[List[str]]\n",
        "    clean_text: str\n",
        "\n",
        "@dataclass\n",
        "class DocumentStructure:\n",
        "    doc_id: str\n",
        "    nome_doc: str\n",
        "    versao: str\n",
        "    data_publicacao: str\n",
        "    pagina_inicial: int\n",
        "    pagina_final: int\n",
        "    paginas: List[PageData]\n",
        "\n",
        "class CompleteDocumentProcessor:\n",
        "    def __init__(self):\n",
        "        self.siglas = {\n",
        "            'TCC': 'Trabalho de Conclus√£o de Curso',\n",
        "            'BSI': 'Bacharelado em Sistemas de Informa√ß√£o',\n",
        "            'ACC': 'Atividades Complementares de Curr√≠culo',\n",
        "            'AC': 'Atividades Complementares',\n",
        "            'PPC': 'Projeto Pedag√≥gico do Curso',\n",
        "            'CH': 'Carga Hor√°ria',\n",
        "            'MEC': 'Minist√©rio da Educa√ß√£o',\n",
        "            'SIGAA': 'Sistema Integrado de Gest√£o de Atividades Acad√™micas',\n",
        "            'CEPE': 'Conselho de Ensino, Pesquisa e Extens√£o',\n",
        "            'CNE': 'Conselho Nacional de Educa√ß√£o',\n",
        "            'LDB': 'Lei de Diretrizes e Bases da Educa√ß√£o Nacional',\n",
        "            'IES': 'Institui√ß√£o de Ensino Superior'\n",
        "        }\n",
        "        self.similarity_threshold = 0.85\n",
        "\n",
        "    def extract_complete_text_with_tables(self, pdf_path: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Extrai texto e tabelas\n",
        "        \"\"\"\n",
        "        pages_data = []\n",
        "\n",
        "        with pdfplumber.open(pdf_path) as pdf:\n",
        "            total_pages = len(pdf.pages)\n",
        "            print(f\"Processando {total_pages} p√°ginas...\")\n",
        "\n",
        "            for page_num, page in enumerate(pdf.pages, 1):\n",
        "\n",
        "                # texto\n",
        "                text = page.extract_text() or \"\"\n",
        "                raw_text = \" \".join(text.split())  # limpeza b√°sica\n",
        "\n",
        "                # tabelas\n",
        "                tables = []\n",
        "                page_tables = page.extract_tables()\n",
        "                if page_tables:\n",
        "                    for table in page_tables:\n",
        "                        if table and any(any(cell for cell in row if cell) for row in table):\n",
        "                            cleaned_table = []\n",
        "                            for row in table:\n",
        "                                cleaned_row = [str(cell).strip() if cell else \"\" for cell in row]\n",
        "                                if any(cleaned_row):  #\n",
        "                                    cleaned_table.append(cleaned_row)\n",
        "                            if cleaned_table:\n",
        "                                tables.append(cleaned_table)\n",
        "\n",
        "                # detectar cap√≠tulo e artigo na p√°gina\n",
        "                chapter, article = self._detect_structure_on_page(raw_text)\n",
        "\n",
        "                # normalizar texto\n",
        "                normalized_text = self.preprocess_text(raw_text)\n",
        "\n",
        "                # texto limpo\n",
        "                clean_text = self.clean_text(raw_text)\n",
        "\n",
        "                pages_data.append({\n",
        "                    \"page\": page_num,\n",
        "                    \"raw_text\": raw_text,\n",
        "                    \"normalized_text\": normalized_text,\n",
        "                    \"chapter\": chapter,\n",
        "                    \"article\": article,\n",
        "                    \"tables\": tables,\n",
        "                    \"clean_text\": clean_text\n",
        "                })\n",
        "\n",
        "                if page_num % 10 == 0:\n",
        "                    print(f\"     P√°ginas processadas: {page_num}/{total_pages}\")\n",
        "\n",
        "        print(f\"     Extra√ß√£o conclu√≠da: {len(pages_data)} p√°ginas\")\n",
        "        return pages_data\n",
        "\n",
        "    def _detect_structure_on_page(self, text: str) -> tuple[Optional[str], Optional[str]]:\n",
        "        \"\"\"\n",
        "        Detecta cap√≠tulo e artigo em uma p√°gina\n",
        "        \"\"\"\n",
        "        chapter = None\n",
        "        article = None\n",
        "\n",
        "        lines = text.split('\\n')\n",
        "\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "\n",
        "            # detectar cap√≠tulo\n",
        "            chapter_match = re.search(\n",
        "                r'(CAP[√çI]TULO|T[√çI]TULO)\\s+([IVXLCDM]+|\\d+(\\.\\d+)*)',\n",
        "                line,\n",
        "                re.IGNORECASE\n",
        "            )\n",
        "            if chapter_match:\n",
        "                chapter = line\n",
        "                continue\n",
        "\n",
        "            # detectar padr√µes num√©ricos como \"6.5\", \"3.2.1\"\n",
        "            numeric_chapter = re.search(r'^\\d+(\\.\\d+)+', line)\n",
        "            if numeric_chapter and not chapter:\n",
        "                chapter = line\n",
        "                continue\n",
        "\n",
        "            # detectar artigo\n",
        "            artigo_match = re.search(r'Art\\.?\\s*\\d+[¬∫¬∞]?\\.?', line)\n",
        "            if artigo_match:\n",
        "                article = artigo_match.group(0)\n",
        "                break\n",
        "\n",
        "        return chapter, article\n",
        "\n",
        "    def preprocess_text(self, text: str) -> str:\n",
        "        \"\"\"\n",
        "        Normaliza√ß√£o do texto para deduplica√ß√£o:\n",
        "        - Min√∫sculas, remo√ß√£o de pontua√ß√£o, etc.\n",
        "        \"\"\"\n",
        "        if not text:\n",
        "            return \"\"\n",
        "\n",
        "        # converter para min√∫sculas\n",
        "        text = text.lower()\n",
        "\n",
        "        # expandir siglas\n",
        "        for sigla, expansao in self.siglas.items():\n",
        "            text = re.sub(rf'\\b{sigla}\\b', expansao.lower(), text, flags=re.IGNORECASE)\n",
        "\n",
        "        # normalizar caracteres (remover acentos)\n",
        "        text = unicodedata.normalize('NFKD', text).encode('ASCII', 'ignore').decode('ASCII')\n",
        "\n",
        "        # remover pontua√ß√£o e caracteres especiais\n",
        "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "\n",
        "        # padronizar formato de artigos e par√°grafos\n",
        "        text = re.sub(r'artigo\\s+(\\d+)', r'art \\1', text)\n",
        "        text = re.sub(r'art\\.?\\s*(\\d+)', r'art \\1', text)\n",
        "        text = re.sub(r'par√°grafo\\s+√∫nico', 'paragrafo unico', text)\n",
        "        text = re.sub(r'¬ß\\s*(\\d+)', r'paragrafo \\1', text)\n",
        "\n",
        "        # remover espa√ßos extras e normalizar\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "        return text\n",
        "\n",
        "    def clean_text(self, text: str) -> str:\n",
        "        \"\"\"\n",
        "        Limpeza b√°sica do texto para deduplica√ß√£o\n",
        "        \"\"\"\n",
        "        if not text:\n",
        "            return \"\"\n",
        "\n",
        "        # remover espa√ßos extras e normalizar quebras\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "        return text\n",
        "\n",
        "    def deduplicate_pages(self, pages_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Remove p√°ginas duplicadas ou muito similares\n",
        "        \"\"\"\n",
        "\n",
        "        if len(pages_data) <= 1:\n",
        "            return pages_data\n",
        "\n",
        "        # deduplica√ß√£o exata\n",
        "        unique_pages = self._remove_exact_duplicates(pages_data)\n",
        "        print(f\"  Fase 1 - Duplicatas exatas: {len(pages_data) - len(unique_pages)} removidas\")\n",
        "\n",
        "        # deduplica√ß√£o por similaridade\n",
        "        final_pages = self._remove_similar_pages(unique_pages)\n",
        "        print(f\"  Fase 2 - P√°ginas similares: {len(unique_pages) - len(final_pages)} removidas\")\n",
        "\n",
        "        return final_pages\n",
        "\n",
        "    def _remove_exact_duplicates(self, pages_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Remove p√°ginas com conte√∫do exatamente igual\"\"\"\n",
        "        unique_pages = []\n",
        "        seen_hashes = set()\n",
        "\n",
        "        for page in pages_data:\n",
        "            # criar hash do conte√∫do normalizado\n",
        "            content_hash = hashlib.md5(page['normalized_text'].encode()).hexdigest()\n",
        "\n",
        "            if content_hash not in seen_hashes:\n",
        "                seen_hashes.add(content_hash)\n",
        "                unique_pages.append(page)\n",
        "            else:\n",
        "                print(f\"    üóëÔ∏è Removendo p√°gina {page['page']} (duplicata exata)\")\n",
        "\n",
        "        return unique_pages\n",
        "\n",
        "    def _remove_similar_pages(self, pages_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Remove p√°ginas com conte√∫do muito similar\"\"\"\n",
        "        if len(pages_data) <= 1:\n",
        "            return pages_data\n",
        "\n",
        "        # extrair textos para compara√ß√£o\n",
        "        texts = [page['normalized_text'] for page in pages_data]\n",
        "\n",
        "        # usar TF-IDF e similaridade de cosseno\n",
        "        vectorizer = TfidfVectorizer(min_df=1, max_df=0.9)\n",
        "        try:\n",
        "            tfidf_matrix = vectorizer.fit_transform(texts)\n",
        "            cosine_sim = cosine_similarity(tfidf_matrix)\n",
        "\n",
        "            to_keep = set(range(len(pages_data)))\n",
        "\n",
        "            for i in range(len(pages_data)):\n",
        "                if i in to_keep:\n",
        "                    for j in range(i + 1, len(pages_data)):\n",
        "                        if j in to_keep and cosine_sim[i][j] >= self.similarity_threshold:\n",
        "                            to_keep.remove(j)\n",
        "                            print(f\"  Removendo p√°gina {pages_data[j]['page']} \"\n",
        "                                  f\"(similar √† p√°gina {pages_data[i]['page']})\")\n",
        "\n",
        "            return [pages_data[i] for i in sorted(to_keep)]\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  Erro na deduplica√ß√£o fuzzy: {e}\")\n",
        "            return pages_data  # retorna todas se houver erro\n",
        "\n",
        "    def process_complete_document(self, pdf_path: str, metadata: Dict[str, str]) -> DocumentStructure:\n",
        "        \"\"\"Processa documento  com todas as p√°ginas no formato solicitado\"\"\"\n",
        "        print(f\"PROCESSANDO DOCUMENTO : {metadata['nome_doc']}\")\n",
        "\n",
        "\n",
        "        print(\"Extraindo todas as p√°ginas com texto e tabelas\")\n",
        "        pages_data = self.extract_complete_text_with_tables(pdf_path)\n",
        "\n",
        "        if not pages_data:\n",
        "            raise ValueError(f\"N√£o foi poss√≠vel extrair texto do documento: {metadata['nome_doc']}\")\n",
        "\n",
        "        # deduplica√ß√£o\n",
        "        final_pages = self.deduplicate_pages(pages_data)\n",
        "\n",
        "        # converter para objetos PageData\n",
        "        paginas_objects = []\n",
        "        for page in final_pages:\n",
        "            paginas_objects.append(PageData(\n",
        "                page=page['page'],\n",
        "                raw_text=page['raw_text'],\n",
        "                normalized_text=page['normalized_text'],\n",
        "                chapter=page['chapter'],\n",
        "                article=page['article'],\n",
        "                tables=page['tables'],\n",
        "                clean_text=page['clean_text']\n",
        "            ))\n",
        "\n",
        "        # criar estrutura final do documento\n",
        "        document = DocumentStructure(\n",
        "            doc_id=metadata['doc_id'],\n",
        "            nome_doc=metadata['nome_doc'],\n",
        "            versao=metadata['versao'],\n",
        "            data_publicacao=metadata['data_publicacao'],\n",
        "            pagina_inicial=1,\n",
        "            pagina_final=len(pages_data),  # numero total de p√°ginas originais\n",
        "            paginas=paginas_objects\n",
        "        )\n",
        "\n",
        "        print(f\"Processamento conclu√≠do: {len(final_pages)} p√°ginas √∫nicas\")\n",
        "        return document\n",
        "\n",
        "def save_to_jsonl(documents: List[DocumentStructure], output_path: str):\n",
        "    \"\"\"Salva documentos no formato JSONL exato solicitado\"\"\"\n",
        "    with open(output_path, 'w', encoding='utf-8') as f:\n",
        "        for doc in documents:\n",
        "\n",
        "            doc_dict = {\n",
        "                \"doc_id\": doc.doc_id,\n",
        "                \"nome_doc\": doc.nome_doc,\n",
        "                \"versao\": doc.versao,\n",
        "                \"data_publicacao\": doc.data_publicacao,\n",
        "                \"pagina_inicial\": doc.pagina_inicial,\n",
        "                \"pagina_final\": doc.pagina_final,\n",
        "                \"paginas\": [\n",
        "                    {\n",
        "                        \"page\": page.page,\n",
        "                        \"raw_text\": page.raw_text,\n",
        "                        \"normalized_text\": page.normalized_text,\n",
        "                        \"chapter\": page.chapter,\n",
        "                        \"article\": page.article,\n",
        "                        \"tables\": page.tables,\n",
        "                        \"clean_text\": page.clean_text\n",
        "                    }\n",
        "                    for page in doc.paginas\n",
        "                ]\n",
        "            }\n",
        "\n",
        "            f.write(json.dumps(doc_dict, ensure_ascii=False) + '\\n')\n",
        "\n",
        "def generate_report(documents: List[DocumentStructure]):\n",
        "    \"\"\"Gera relat√≥rio  dos documentos processados\"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    total_paginas = 0\n",
        "    total_tabelas = 0\n",
        "\n",
        "    for doc in documents:\n",
        "        doc_paginas = len(doc.paginas)\n",
        "        doc_tabelas = sum(len(page.tables) for page in doc.paginas)\n",
        "        total_paginas += doc_paginas\n",
        "        total_tabelas += doc_tabelas\n",
        "\n",
        "        print(f\"\\n {doc.nome_doc}\")\n",
        "        print(f\"    P√°ginas processadas: {doc_paginas}\")\n",
        "        print(f\"    Tabelas extra√≠das: {doc_tabelas}\")\n",
        "        print(f\"    ID: {doc.doc_id}\")\n",
        "        print(f\"    Vers√£o: {doc.versao} - {doc.data_publicacao}\")\n",
        "\n",
        "    print(f\"\\n TOTAL GERAL:\")\n",
        "    print(f\"    Documentos: {len(documents)}\")\n",
        "    print(f\"    P√°ginas: {total_paginas}\")\n",
        "    print(f\"    Tabelas: {total_tabelas}\")\n",
        "\n",
        "def process_all_documents():\n",
        "    \"\"\"Processa os documentos\"\"\"\n",
        "    processor = CompleteDocumentProcessor()\n",
        "    documents = []\n",
        "\n",
        "    documentos_config = [\n",
        "        {\n",
        "            'path': '/content/sample_data/regulamento_tcc.pdf',\n",
        "            'metadata': {\n",
        "                'doc_id': 'TCC_BSI_2024_001',\n",
        "                'nome_doc': 'REGULAMENTO TCC BSI',\n",
        "                'versao': '1.0',\n",
        "                'data_publicacao': '2024-01-15'\n",
        "            }\n",
        "        },\n",
        "        {\n",
        "            'path': '/content/sample_data/estatuto_2025.pdf',\n",
        "            'metadata': {\n",
        "                'doc_id': 'ESTATUTO_2025_001',\n",
        "                'nome_doc': 'Estatuto - Setembro de 2025',\n",
        "                'versao': '2.0',\n",
        "                'data_publicacao': '2025-09-01'\n",
        "            }\n",
        "        },\n",
        "        {\n",
        "            'path': '/content/sample_data/regulamento_atividades_complementares.pdf',\n",
        "            'metadata': {\n",
        "                'doc_id': 'ACC_2024_001',\n",
        "                'nome_doc': 'REGULAMENTO DAS ATIVIDADES COMPLEMENTARES',\n",
        "                'versao': '1.1',\n",
        "                'data_publicacao': '2024-03-20'\n",
        "            }\n",
        "        }\n",
        "    ]\n",
        "\n",
        "\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    for doc_config in documentos_config:\n",
        "        try:\n",
        "            print(f\"\\n Processando: {doc_config['metadata']['nome_doc']}\")\n",
        "            document = processor.process_complete_document(\n",
        "                doc_config['path'],\n",
        "                doc_config['metadata']\n",
        "            )\n",
        "            documents.append(document)\n",
        "            print(f\"Conclu√≠do: {doc_config['metadata']['nome_doc']}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao processar {doc_config['metadata']['nome_doc']}: {e}\")\n",
        "\n",
        "    # salvar resultados\n",
        "    if documents:\n",
        "\n",
        "        output_file = f\"output.jsonl\"\n",
        "\n",
        "        save_to_jsonl(documents, output_file)\n",
        "\n",
        "        # gerar relat√≥rio\n",
        "        generate_report(documents)\n",
        "\n",
        "        print(f\"\\nArquivo salvo: {output_file}\")\n",
        "\n",
        "    return documents\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    documents = process_all_documents()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
