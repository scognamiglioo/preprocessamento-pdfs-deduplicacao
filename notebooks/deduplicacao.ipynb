{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "nBzRmiVFRvG6"
      },
      "outputs": [],
      "source": [
        "!pip install pdfplumber -q\n",
        "!pip install PyPDF2 -q\n",
        "!pip install python-Levenshtein -q\n",
        "!pip install scikit-learn -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "HUgv_6NyOKk5"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import json\n",
        "import hashlib\n",
        "from typing import List, Dict, Any, Optional\n",
        "from dataclasses import dataclass\n",
        "from datetime import datetime\n",
        "import unicodedata\n",
        "from difflib import SequenceMatcher\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "from PyPDF2 import PdfReader\n",
        "import pdfplumber"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FjAZRlWTIUfh",
        "outputId": "96e0378c-0b72-4c1a-964b-a954ca045928"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "\n",
            " Processando: REGULAMENTO TCC BSI\n",
            "PROCESSANDO DOCUMENTO COMPLETO: REGULAMENTO TCC BSI\n",
            "Extraindo todas as páginas com texto e tabelas\n",
            "Processando 13 páginas...\n",
            "     Páginas processadas: 10/13\n",
            "     Extração concluída: 13 páginas\n",
            "  Fase 1 - Duplicatas exatas: 0 removidas\n",
            "  Removendo página 12 (similar à página 11)\n",
            "  Fase 2 - Páginas similares: 1 removidas\n",
            "Processamento concluído: 12 páginas únicas\n",
            "Concluído: REGULAMENTO TCC BSI\n",
            "\n",
            " Processando: Estatuto - Setembro de 2025\n",
            "PROCESSANDO DOCUMENTO COMPLETO: Estatuto - Setembro de 2025\n",
            "Extraindo todas as páginas com texto e tabelas\n",
            "Processando 54 páginas...\n",
            "     Páginas processadas: 10/54\n",
            "     Páginas processadas: 20/54\n",
            "     Páginas processadas: 30/54\n",
            "     Páginas processadas: 40/54\n",
            "     Páginas processadas: 50/54\n",
            "     Extração concluída: 54 páginas\n",
            "  Fase 1 - Duplicatas exatas: 0 removidas\n",
            "  Fase 2 - Páginas similares: 0 removidas\n",
            "Processamento concluído: 54 páginas únicas\n",
            "Concluído: Estatuto - Setembro de 2025\n",
            "\n",
            " Processando: REGULAMENTO DAS ATIVIDADES COMPLEMENTARES\n",
            "PROCESSANDO DOCUMENTO COMPLETO: REGULAMENTO DAS ATIVIDADES COMPLEMENTARES\n",
            "Extraindo todas as páginas com texto e tabelas\n",
            "Processando 8 páginas...\n",
            "     Extração concluída: 8 páginas\n",
            "  Fase 1 - Duplicatas exatas: 0 removidas\n",
            "  Fase 2 - Páginas similares: 0 removidas\n",
            "Processamento concluído: 8 páginas únicas\n",
            "Concluído: REGULAMENTO DAS ATIVIDADES COMPLEMENTARES\n",
            "\n",
            "======================================================================\n",
            "======================================================================\n",
            "\n",
            " REGULAMENTO TCC BSI\n",
            "    Páginas processadas: 12\n",
            "    Tabelas extraídas: 3\n",
            "    ID: TCC_BSI_2024_001\n",
            "    Versão: 1.0 - 2024-01-15\n",
            "\n",
            " Estatuto - Setembro de 2025\n",
            "    Páginas processadas: 54\n",
            "    Tabelas extraídas: 1\n",
            "    ID: ESTATUTO_2025_001\n",
            "    Versão: 2.0 - 2025-09-01\n",
            "\n",
            " REGULAMENTO DAS ATIVIDADES COMPLEMENTARES\n",
            "    Páginas processadas: 8\n",
            "    Tabelas extraídas: 6\n",
            "    ID: ACC_2024_001\n",
            "    Versão: 1.1 - 2024-03-20\n",
            "\n",
            " TOTAL GERAL:\n",
            "    Documentos: 3\n",
            "    Páginas: 74\n",
            "    Tabelas: 10\n",
            "\n",
            "Arquivo salvo: output.jsonl\n"
          ]
        }
      ],
      "source": [
        "@dataclass\n",
        "class PageData:\n",
        "    page: int\n",
        "    raw_text: str\n",
        "    normalized_text: str\n",
        "    chapter: Optional[str]\n",
        "    article: Optional[str]\n",
        "    tables: List[List[str]]\n",
        "    clean_text: str\n",
        "\n",
        "@dataclass\n",
        "class DocumentStructure:\n",
        "    doc_id: str\n",
        "    nome_doc: str\n",
        "    versao: str\n",
        "    data_publicacao: str\n",
        "    pagina_inicial: int\n",
        "    pagina_final: int\n",
        "    paginas: List[PageData]\n",
        "\n",
        "class CompleteDocumentProcessor:\n",
        "    def __init__(self):\n",
        "        self.siglas = {\n",
        "            'TCC': 'Trabalho de Conclusão de Curso',\n",
        "            'BSI': 'Bacharelado em Sistemas de Informação',\n",
        "            'ACC': 'Atividades Complementares de Currículo',\n",
        "            'AC': 'Atividades Complementares',\n",
        "            'PPC': 'Projeto Pedagógico do Curso',\n",
        "            'CH': 'Carga Horária',\n",
        "            'MEC': 'Ministério da Educação',\n",
        "            'SIGAA': 'Sistema Integrado de Gestão de Atividades Acadêmicas',\n",
        "            'CEPE': 'Conselho de Ensino, Pesquisa e Extensão',\n",
        "            'CNE': 'Conselho Nacional de Educação',\n",
        "            'LDB': 'Lei de Diretrizes e Bases da Educação Nacional',\n",
        "            'IES': 'Instituição de Ensino Superior'\n",
        "        }\n",
        "        self.similarity_threshold = 0.85\n",
        "\n",
        "    def extract_complete_text_with_tables(self, pdf_path: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Extrai texto e tabelas\n",
        "        \"\"\"\n",
        "        pages_data = []\n",
        "\n",
        "        with pdfplumber.open(pdf_path) as pdf:\n",
        "            total_pages = len(pdf.pages)\n",
        "            print(f\"Processando {total_pages} páginas...\")\n",
        "\n",
        "            for page_num, page in enumerate(pdf.pages, 1):\n",
        "\n",
        "                # texto\n",
        "                text = page.extract_text() or \"\"\n",
        "                raw_text = \" \".join(text.split())  # limpeza básica\n",
        "\n",
        "                # tabelas\n",
        "                tables = []\n",
        "                page_tables = page.extract_tables()\n",
        "                if page_tables:\n",
        "                    for table in page_tables:\n",
        "                        if table and any(any(cell for cell in row if cell) for row in table):\n",
        "                            cleaned_table = []\n",
        "                            for row in table:\n",
        "                                cleaned_row = [str(cell).strip() if cell else \"\" for cell in row]\n",
        "                                if any(cleaned_row):  #\n",
        "                                    cleaned_table.append(cleaned_row)\n",
        "                            if cleaned_table:\n",
        "                                tables.append(cleaned_table)\n",
        "\n",
        "                # detectar capítulo e artigo na página\n",
        "                chapter, article = self._detect_structure_on_page(raw_text)\n",
        "\n",
        "                # normalizar texto\n",
        "                normalized_text = self.preprocess_text(raw_text)\n",
        "\n",
        "                # texto limpo\n",
        "                clean_text = self.clean_text(raw_text)\n",
        "\n",
        "                pages_data.append({\n",
        "                    \"page\": page_num,\n",
        "                    \"raw_text\": raw_text,\n",
        "                    \"normalized_text\": normalized_text,\n",
        "                    \"chapter\": chapter,\n",
        "                    \"article\": article,\n",
        "                    \"tables\": tables,\n",
        "                    \"clean_text\": clean_text\n",
        "                })\n",
        "\n",
        "                if page_num % 10 == 0:\n",
        "                    print(f\"     Páginas processadas: {page_num}/{total_pages}\")\n",
        "\n",
        "        print(f\"     Extração concluída: {len(pages_data)} páginas\")\n",
        "        return pages_data\n",
        "\n",
        "    def _detect_structure_on_page(self, text: str) -> tuple[Optional[str], Optional[str]]:\n",
        "        \"\"\"\n",
        "        Detecta capítulo e artigo em uma página\n",
        "        \"\"\"\n",
        "        chapter = None\n",
        "        article = None\n",
        "\n",
        "        lines = text.split('\\n')\n",
        "\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "\n",
        "            # detectar capítulo\n",
        "            chapter_match = re.search(\n",
        "                r'(CAP[ÍI]TULO|T[ÍI]TULO)\\s+([IVXLCDM]+|\\d+(\\.\\d+)*)',\n",
        "                line,\n",
        "                re.IGNORECASE\n",
        "            )\n",
        "            if chapter_match:\n",
        "                chapter = line\n",
        "                continue\n",
        "\n",
        "            # detectar padrões numéricos como \"6.5\", \"3.2.1\"\n",
        "            numeric_chapter = re.search(r'^\\d+(\\.\\d+)+', line)\n",
        "            if numeric_chapter and not chapter:\n",
        "                chapter = line\n",
        "                continue\n",
        "\n",
        "            # detectar artigo\n",
        "            artigo_match = re.search(r'Art\\.?\\s*\\d+[º°]?\\.?', line)\n",
        "            if artigo_match:\n",
        "                article = artigo_match.group(0)\n",
        "                break\n",
        "\n",
        "        return chapter, article\n",
        "\n",
        "    def preprocess_text(self, text: str) -> str:\n",
        "        \"\"\"\n",
        "        Normalização do texto para deduplicação:\n",
        "        - Minúsculas, remoção de pontuação, etc.\n",
        "        \"\"\"\n",
        "        if not text:\n",
        "            return \"\"\n",
        "\n",
        "        # converter para minúsculas\n",
        "        text = text.lower()\n",
        "\n",
        "        # expandir siglas\n",
        "        for sigla, expansao in self.siglas.items():\n",
        "            text = re.sub(rf'\\b{sigla}\\b', expansao.lower(), text, flags=re.IGNORECASE)\n",
        "\n",
        "        # normalizar caracteres (remover acentos)\n",
        "        text = unicodedata.normalize('NFKD', text).encode('ASCII', 'ignore').decode('ASCII')\n",
        "\n",
        "        # remover pontuação e caracteres especiais\n",
        "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "\n",
        "        # padronizar formato de artigos e parágrafos\n",
        "        text = re.sub(r'artigo\\s+(\\d+)', r'art \\1', text)\n",
        "        text = re.sub(r'art\\.?\\s*(\\d+)', r'art \\1', text)\n",
        "        text = re.sub(r'parágrafo\\s+único', 'paragrafo unico', text)\n",
        "        text = re.sub(r'§\\s*(\\d+)', r'paragrafo \\1', text)\n",
        "\n",
        "        # remover espaços extras e normalizar\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "        return text\n",
        "\n",
        "    def clean_text(self, text: str) -> str:\n",
        "        \"\"\"\n",
        "        Limpeza básica do texto para deduplicação\n",
        "        \"\"\"\n",
        "        if not text:\n",
        "            return \"\"\n",
        "\n",
        "        # remover espaços extras e normalizar quebras\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "        return text\n",
        "\n",
        "    def deduplicate_pages(self, pages_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Remove páginas duplicadas ou muito similares\n",
        "        \"\"\"\n",
        "\n",
        "        if len(pages_data) <= 1:\n",
        "            return pages_data\n",
        "\n",
        "        # deduplicação exata\n",
        "        unique_pages = self._remove_exact_duplicates(pages_data)\n",
        "        print(f\"  Fase 1 - Duplicatas exatas: {len(pages_data) - len(unique_pages)} removidas\")\n",
        "\n",
        "        # deduplicação por similaridade\n",
        "        final_pages = self._remove_similar_pages(unique_pages)\n",
        "        print(f\"  Fase 2 - Páginas similares: {len(unique_pages) - len(final_pages)} removidas\")\n",
        "\n",
        "        return final_pages\n",
        "\n",
        "    def _remove_exact_duplicates(self, pages_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Remove páginas com conteúdo exatamente igual\"\"\"\n",
        "        unique_pages = []\n",
        "        seen_hashes = set()\n",
        "\n",
        "        for page in pages_data:\n",
        "            # criar hash do conteúdo normalizado\n",
        "            content_hash = hashlib.md5(page['normalized_text'].encode()).hexdigest()\n",
        "\n",
        "            if content_hash not in seen_hashes:\n",
        "                seen_hashes.add(content_hash)\n",
        "                unique_pages.append(page)\n",
        "            else:\n",
        "                print(f\"  Removendo página {page['page']} (duplicata exata)\")\n",
        "\n",
        "        return unique_pages\n",
        "\n",
        "    def _remove_similar_pages(self, pages_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Remove páginas com conteúdo muito similar\"\"\"\n",
        "        if len(pages_data) <= 1:\n",
        "            return pages_data\n",
        "\n",
        "        # extrair textos para comparação\n",
        "        texts = [page['normalized_text'] for page in pages_data]\n",
        "\n",
        "        # usar TF-IDF e similaridade de cosseno\n",
        "        vectorizer = TfidfVectorizer(min_df=1, max_df=0.9)\n",
        "        try:\n",
        "            tfidf_matrix = vectorizer.fit_transform(texts)\n",
        "            cosine_sim = cosine_similarity(tfidf_matrix)\n",
        "\n",
        "            to_keep = set(range(len(pages_data)))\n",
        "\n",
        "            for i in range(len(pages_data)):\n",
        "                if i in to_keep:\n",
        "                    for j in range(i + 1, len(pages_data)):\n",
        "                        if j in to_keep and cosine_sim[i][j] >= self.similarity_threshold:\n",
        "                            to_keep.remove(j)\n",
        "                            print(f\"  Removendo página {pages_data[j]['page']} \"\n",
        "                                  f\"(similar à página {pages_data[i]['page']})\")\n",
        "\n",
        "            return [pages_data[i] for i in sorted(to_keep)]\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  Erro na deduplicação fuzzy: {e}\")\n",
        "            return pages_data  # retorna todas se houver erro\n",
        "\n",
        "    def process_complete_document(self, pdf_path: str, metadata: Dict[str, str]) -> DocumentStructure:\n",
        "        \"\"\"Processa documento  com todas as páginas no formato solicitado\"\"\"\n",
        "        print(f\"PROCESSANDO DOCUMENTO : {metadata['nome_doc']}\")\n",
        "\n",
        "\n",
        "        print(\"Extraindo todas as páginas com texto e tabelas\")\n",
        "        pages_data = self.extract_complete_text_with_tables(pdf_path)\n",
        "\n",
        "        if not pages_data:\n",
        "            raise ValueError(f\"Não foi possível extrair texto do documento: {metadata['nome_doc']}\")\n",
        "\n",
        "        # deduplicação\n",
        "        final_pages = self.deduplicate_pages(pages_data)\n",
        "\n",
        "        # converter para objetos PageData\n",
        "        paginas_objects = []\n",
        "        for page in final_pages:\n",
        "            paginas_objects.append(PageData(\n",
        "                page=page['page'],\n",
        "                raw_text=page['raw_text'],\n",
        "                normalized_text=page['normalized_text'],\n",
        "                chapter=page['chapter'],\n",
        "                article=page['article'],\n",
        "                tables=page['tables'],\n",
        "                clean_text=page['clean_text']\n",
        "            ))\n",
        "\n",
        "        # criar estrutura final do documento\n",
        "        document = DocumentStructure(\n",
        "            doc_id=metadata['doc_id'],\n",
        "            nome_doc=metadata['nome_doc'],\n",
        "            versao=metadata['versao'],\n",
        "            data_publicacao=metadata['data_publicacao'],\n",
        "            pagina_inicial=1,\n",
        "            pagina_final=len(pages_data),  # numero total de páginas originais\n",
        "            paginas=paginas_objects\n",
        "        )\n",
        "\n",
        "        print(f\"Processamento concluído: {len(final_pages)} páginas únicas\")\n",
        "        return document\n",
        "\n",
        "def save_to_jsonl(documents: List[DocumentStructure], output_path: str):\n",
        "    \"\"\"Salva documentos no formato JSONL exato solicitado\"\"\"\n",
        "    with open(output_path, 'w', encoding='utf-8') as f:\n",
        "        for doc in documents:\n",
        "\n",
        "            doc_dict = {\n",
        "                \"doc_id\": doc.doc_id,\n",
        "                \"nome_doc\": doc.nome_doc,\n",
        "                \"versao\": doc.versao,\n",
        "                \"data_publicacao\": doc.data_publicacao,\n",
        "                \"pagina_inicial\": doc.pagina_inicial,\n",
        "                \"pagina_final\": doc.pagina_final,\n",
        "                \"paginas\": [\n",
        "                    {\n",
        "                        \"page\": page.page,\n",
        "                        \"raw_text\": page.raw_text,\n",
        "                        \"normalized_text\": page.normalized_text,\n",
        "                        \"chapter\": page.chapter,\n",
        "                        \"article\": page.article,\n",
        "                        \"tables\": page.tables,\n",
        "                        \"clean_text\": page.clean_text\n",
        "                    }\n",
        "                    for page in doc.paginas\n",
        "                ]\n",
        "            }\n",
        "\n",
        "            f.write(json.dumps(doc_dict, ensure_ascii=False) + '\\n')\n",
        "\n",
        "def generate_report(documents: List[DocumentStructure]):\n",
        "    \"\"\"Gera relatório  dos documentos processados\"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    total_paginas = 0\n",
        "    total_tabelas = 0\n",
        "\n",
        "    for doc in documents:\n",
        "        doc_paginas = len(doc.paginas)\n",
        "        doc_tabelas = sum(len(page.tables) for page in doc.paginas)\n",
        "        total_paginas += doc_paginas\n",
        "        total_tabelas += doc_tabelas\n",
        "\n",
        "        print(f\"\\n {doc.nome_doc}\")\n",
        "        print(f\"    Páginas processadas: {doc_paginas}\")\n",
        "        print(f\"    Tabelas extraídas: {doc_tabelas}\")\n",
        "        print(f\"    ID: {doc.doc_id}\")\n",
        "        print(f\"    Versão: {doc.versao} - {doc.data_publicacao}\")\n",
        "\n",
        "    print(f\"\\n TOTAL GERAL:\")\n",
        "    print(f\"    Documentos: {len(documents)}\")\n",
        "    print(f\"    Páginas: {total_paginas}\")\n",
        "    print(f\"    Tabelas: {total_tabelas}\")\n",
        "\n",
        "def process_all_documents():\n",
        "    \"\"\"Processa os documentos\"\"\"\n",
        "    processor = CompleteDocumentProcessor()\n",
        "    documents = []\n",
        "\n",
        "    documentos_config = [\n",
        "        {\n",
        "            'path': '/content/sample_data/regulamento_tcc.pdf',\n",
        "            'metadata': {\n",
        "                'doc_id': 'TCC_BSI_2024_001',\n",
        "                'nome_doc': 'REGULAMENTO TCC BSI',\n",
        "                'versao': '1.0',\n",
        "                'data_publicacao': '2024-01-15'\n",
        "            }\n",
        "        },\n",
        "        {\n",
        "            'path': '/content/sample_data/estatuto_2025.pdf',\n",
        "            'metadata': {\n",
        "                'doc_id': 'ESTATUTO_2025_001',\n",
        "                'nome_doc': 'Estatuto - Setembro de 2025',\n",
        "                'versao': '2.0',\n",
        "                'data_publicacao': '2025-09-01'\n",
        "            }\n",
        "        },\n",
        "        {\n",
        "            'path': '/content/sample_data/regulamento_atividades_complementares.pdf',\n",
        "            'metadata': {\n",
        "                'doc_id': 'ACC_2024_001',\n",
        "                'nome_doc': 'REGULAMENTO DAS ATIVIDADES COMPLEMENTARES',\n",
        "                'versao': '1.1',\n",
        "                'data_publicacao': '2024-03-20'\n",
        "            }\n",
        "        }\n",
        "    ]\n",
        "\n",
        "\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    for doc_config in documentos_config:\n",
        "        try:\n",
        "            print(f\"\\n Processando: {doc_config['metadata']['nome_doc']}\")\n",
        "            document = processor.process_complete_document(\n",
        "                doc_config['path'],\n",
        "                doc_config['metadata']\n",
        "            )\n",
        "            documents.append(document)\n",
        "            print(f\"Concluído: {doc_config['metadata']['nome_doc']}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao processar {doc_config['metadata']['nome_doc']}: {e}\")\n",
        "\n",
        "    # salvar resultados\n",
        "    if documents:\n",
        "\n",
        "        output_file = f\"output.jsonl\"\n",
        "\n",
        "        save_to_jsonl(documents, output_file)\n",
        "\n",
        "        # gerar relatório\n",
        "        generate_report(documents)\n",
        "\n",
        "        print(f\"\\nArquivo salvo: {output_file}\")\n",
        "\n",
        "    return documents\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    documents = process_all_documents()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
